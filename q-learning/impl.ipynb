{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "import torch.nn.functional as F\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from torch.autograd import Variable"
      ],
      "outputs": [],
      "metadata": {
        "id": "VEYO1NpspSHU",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "np.random.seed(0)\r\n",
        "num_objects = 10\r\n",
        "num_stimuli = 4\r\n",
        "\r\n",
        "class Shadlen:\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.actions_correctness = []\r\n",
        "        self.step = 0\r\n",
        "        self.criterion = 0.5\r\n",
        "        self.stimuli = self.generate_stimuli(num_stimuli)\r\n",
        "        self.state = self.to_one_hot([self.step] + self.stimuli)\r\n",
        "\r\n",
        "    def to_one_hot(self, array, N=10):\r\n",
        "        array = np.array(array).astype(int)\r\n",
        "        labels = np.zeros((array.shape[0], N), dtype=np.float32)\r\n",
        "        labels[np.arange(array.shape[0]), array] = 1.\r\n",
        "        return labels\r\n",
        "\r\n",
        "    def generate_stimuli(self, N_s):\r\n",
        "        stimuli = []\r\n",
        "        for i in range(N_s):\r\n",
        "            stimuli.append(np.random.randint(0, num_objects))\r\n",
        "        return stimuli\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.__init__()\r\n",
        "        return self.state\r\n",
        "\r\n",
        "    def response(self, action):\r\n",
        "        if self.step == 1:\r\n",
        "            GT = float((np.sum(self.stimuli) / (num_objects * num_stimuli)) < self.criterion)\r\n",
        "        else:\r\n",
        "            GT = float((np.sum(self.stimuli) / (num_objects * num_stimuli)) > self.criterion)\r\n",
        "\r\n",
        "        self.actions_correctness.append(GT == action)\r\n",
        "        self.step += 1\r\n",
        "\r\n",
        "        finished = False\r\n",
        "\r\n",
        "        if self.step == 1:\r\n",
        "            reward = 0.\r\n",
        "\r\n",
        "        elif self.step == 2:\r\n",
        "            if self.actions_correctness[-2]:\r\n",
        "                reward = 0.\r\n",
        "            else:\r\n",
        "                finished = True\r\n",
        "                if self.actions_correctness[-1]:\r\n",
        "                    reward = 1.2\r\n",
        "                else:\r\n",
        "                    reward = -2.\r\n",
        "\r\n",
        "        elif self.step == 3:\r\n",
        "            finished = True\r\n",
        "\r\n",
        "            if self.actions_correctness[-2]:\r\n",
        "                if self.actions_correctness[-1]:\r\n",
        "                    reward = 3.5\r\n",
        "                else:\r\n",
        "                    reward = 0.5\r\n",
        "            else:\r\n",
        "                if self.actions_correctness[-1]:\r\n",
        "                    reward = 0.\r\n",
        "                else:\r\n",
        "                    reward = -1.\r\n",
        "        else:\r\n",
        "            print(\"StepErr!\")\r\n",
        "            return\r\n",
        "\r\n",
        "        self.stimuli = self.generate_stimuli(num_stimuli)\r\n",
        "        self.state = self.to_one_hot([self.step] + self.stimuli)\r\n",
        "\r\n",
        "        return reward, self.state, finished\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "u9DeDdQ4pY5V",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# we use the default LSTM, number of input featurs is the inputs of a 5*10 matrix and we choose the number\r\n",
        "# of hidden layers to be a 'fitting' number, for now we choose 200\r\n",
        "# to compute Q we need a linear operation to be performed on the hidden states of the LSTM, we implement it directly.\r\n",
        "\r\n",
        "input_feature = 10\r\n",
        "number_of_hidden = 100\r\n",
        "output_feature = 2\r\n",
        "number_of_layers = 3\r\n",
        "\r\n",
        "emulator = Shadlen()\r\n",
        "\r\n",
        "class MonkeyLSTM(nn.Module):\r\n",
        "    def __init__(self, input_feature, hidden_feature, output_feature, number_of_layers):\r\n",
        "        super(MonkeyLSTM, self).__init__()\r\n",
        "        self.input_feature = input_feature\r\n",
        "        self.hidden_feature = hidden_feature\r\n",
        "        self.output_feature = output_feature\r\n",
        "        self.layers = number_of_layers\r\n",
        "        self.make_q = nn.Linear(hidden_feature, output_feature)\r\n",
        "        self.lstm = nn.LSTM(input_feature, hidden_feature, number_of_layers)\r\n",
        "        self.reset_hiddens()\r\n",
        "        \r\n",
        "    def forward(self, input_data):\r\n",
        "        input_data = input_data.view(5, 1, -1) # pytorch LSTM expects 3D tensors\r\n",
        "        output, (h_t_next, c_t_next) = self.lstm(input_data, (self.h_t, self.c_t))\r\n",
        "        q_t = self.make_q(h_t_next[-1])\r\n",
        "        \r\n",
        "        return q_t\r\n",
        "        \r\n",
        "    def reset_hiddens(self):\r\n",
        "        self.h_t = torch.zeros(self.layers, 1, self.hidden_feature)\r\n",
        "        self.c_t = torch.zeros(self.layers, 1, self.hidden_feature)\r\n",
        "\r\n",
        "class Monkey:\r\n",
        "    def __init__(self, gamma, epsilon_0):\r\n",
        "        self.gamma = gamma\r\n",
        "        self.epsilon_0 = epsilon_0\r\n",
        "        self.terminal = False\r\n",
        "        self.action = 0\r\n",
        "        self.reward = 0\r\n",
        "        \r\n",
        "    def init_monkey(self, init_state):\r\n",
        "        self.state = torch.from_numpy(init_state) # we use the emulator initial state for the Monkey\r\n",
        "        self.terminal = False\r\n",
        "        \r\n",
        "    def play(self, number_of_episodes, window, lr):\r\n",
        "        \r\n",
        "        loss_list = list()\r\n",
        "        reward_list = list()\r\n",
        "        running_reward = 0.0\r\n",
        "        optimizer = optim.Adam(monkey_lstm.parameters(), lr)\r\n",
        "        criterion = nn.MSELoss()\r\n",
        "        loss_end = 0.0\r\n",
        "        for episode in range(number_of_episodes):\r\n",
        "            state = torch.from_numpy(emulator.reset())\r\n",
        "            monkey.lessGreedy(episode) \r\n",
        "            \r\n",
        "            if(episode % window == 0):\r\n",
        "                reward_list.append(running_reward / window)\r\n",
        "                running_reward = 0.0\r\n",
        "                loss_list.append(loss_end)\r\n",
        "\r\n",
        "            finished = False\r\n",
        "            \r\n",
        "            while finished is False:\r\n",
        "                monkey_lstm.reset_hiddens()\r\n",
        "                optimizer.zero_grad()\r\n",
        "\r\n",
        "                q = monkey_lstm.forward(state.view(5, 1, -1))\r\n",
        "                p = np.random.rand()\r\n",
        "\r\n",
        "                if p < self.epsilon:\r\n",
        "                    action = np.random.randint(0, 2)\r\n",
        "                else:\r\n",
        "                    if(q[0, 0] < q[0, 1]):\r\n",
        "                        action = 1\r\n",
        "                    else:\r\n",
        "                        action = 0\r\n",
        "\r\n",
        "                reward, next_state, finished = emulator.response(action)\r\n",
        "                next_state = torch.from_numpy(next_state).view(5, 1, -1)\r\n",
        "                monkey_lstm.reset_hiddens()\r\n",
        "                q_next = monkey_lstm.forward(next_state.view(5, 1, -1))\r\n",
        "\r\n",
        "                if finished:\r\n",
        "                    y = torch.tensor(reward)\r\n",
        "                else:\r\n",
        "                    if(q_next[0, 0] < q_next[0, 1]):\r\n",
        "                        y = torch.tensor(reward) + self.gamma * q_next[0, 1]\r\n",
        "                    else:\r\n",
        "                        y = torch.tensor(reward) + self.gamma * q_next[0, 0]\r\n",
        "\r\n",
        "                state = next_state\r\n",
        "                \r\n",
        "                loss = criterion(y, q[0, action])\r\n",
        "                loss.backward()\r\n",
        "                optimizer.step() \r\n",
        "                \r\n",
        "                running_reward = running_reward + reward\r\n",
        "        \r\n",
        "                if finished is True:\r\n",
        "                    loss_end = loss\r\n",
        "                \r\n",
        "        plt.plot(reward_list)\r\n",
        "        plt.title('Mean reward')\r\n",
        "        plt.figure()\r\n",
        "        plt.plot(loss_list)\r\n",
        "        plt.title('Loss')\r\n",
        "        \r\n",
        "    def lessGreedy(self, episode):\r\n",
        "        self.epsilon = (self.epsilon_0) * np.exp(-episode / 1000)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-5rYAlXwplb9",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "monkey_lstm = MonkeyLSTM(input_feature=10, hidden_feature=100, output_feature=2, number_of_layers=3)\r\n",
        "monkey = Monkey(0.4, 0.6)\r\n",
        "monkey.play(number_of_episodes=20000, window=200, lr=1e-3)"
      ],
      "outputs": [],
      "metadata": {
        "id": "XTZ2-85EqpCl",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "reward_test = list()\r\n",
        "\r\n",
        "for test in range(500):\r\n",
        "    reward_test.append(monkey.reward)\r\n",
        "    init_state = emulator.reset()\r\n",
        "    monkey.init_monkey(init_state)\r\n",
        "    while monkey.terminal is False:\r\n",
        "        monkey_lstm.reset_hiddens()\r\n",
        "        monkey.play()\r\n",
        "        \r\n",
        "plt.plot(reward_test)\r\n",
        "        "
      ],
      "outputs": [],
      "metadata": {
        "id": "QoTdCtRtA-0C",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "x = list()\r\n",
        "for i in range(100):\r\n",
        "  x.append(np.exp(i/100))\r\n",
        "plt.plot(x)\r\n",
        "plt.title('title with \\Alpha = and %epsilon=0.5')"
      ],
      "outputs": [],
      "metadata": {
        "id": "8Jcn5gDVPhK6",
        "colab_type": "code",
        "colab": {}
      }
    }
  ]
}